# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JN0tuaij7kbt1ONEEyqbhlZDeg1TMAnh
"""

text = '''Abstractive text summarization generates legible sentences from the entirety of the text provided. It rewrites large amounts of text by creating acceptable representations, which is further processed and summarized by natural language processing.
What makes this method unique is its almost AI-like ability to use a machineâ€™s semantic capability to process text and iron out the kinks using NLP.
Although it might not be as simple to use compared to the extractive method, in many situations, abstract summarization is far more useful. In a lot of ways, it is a precursor to full-fledged AI writing tools. However, this does not mean that there is no need for extractive summarization. '''

!pip install -U spacy 
!python -m spacy download en_core_web_sm

import spacy 
from spacy.lang.en.stop_words import STOP_WORDS 
from string import punctuation

stopwords = list(STOP_WORDS) 
stopwords

nlp = spacy.load('en_core_web_sm')
doc = nlp (text)

tokens = [token.text for token in doc]
print(tokens)

punctuation = punctuation  + '\n'
punctuation

word_frequencies = {}
for word in doc:
  if word.text.lower() not in stopwords:
    if word.text.lower() not in punctuation:
      if word.text not in word_frequencies.keys(): 
          word_frequencies[word.text] = 1
      else: 
          word_frequencies[word.text] += 1

print(word_frequencies)

max_frequency = max(word_frequencies.values())

print(max_frequency)

for word in word_frequencies.keys():
   word_frequencies[word] = word_frequencies[word]/max_frequency

print(word_frequencies)

sentence_tokens = [sent for sent in doc.sents]
print(sentence_tokens)

sentence_scores = {}
for sent in sentence_tokens:
   for word in sent:
      if word.text.lower() in word_frequencies.keys():
        if sent not in sentence_scores.keys():
            sentence_scores [sent] = word_frequencies [word.text.lower()]
        else:
            sentence_scores [sent] += word_frequencies [word.text.lower()]

sentence_scores

from heapq import nlargest
select_length = int(len(sentence_tokens)*0.3) 
select_length

summary = nlargest (select_length, sentence_scores, key = sentence_scores.get)

summary

final_summary = [word.text for word in summary]
final_summary
summary =' '.join(final_summary)
summary